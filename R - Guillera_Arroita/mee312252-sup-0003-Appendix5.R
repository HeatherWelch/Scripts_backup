#---------------------------------------------------------------------------------------------------------
# Maxent (including post-processing)
#---------------------------------------------------------------------------------------------------------
# Usage:
#   Features: allow "lq", "lqh" or "default" (using those names)
#   PAbg = T: uses bg from the PAdataset as per Thibaud
#   PAbg = F: uses random sample for background (extra code below for making that from the object L made in other code of Thibaud et al.)
#   outType: which output is to be calculated and returned (1-3 involved post-processing to reflect probabilities)
#            0 = default (logistic not adjusted, as per Thibaud), 1 = raw adjusted, 2 = cloglog adjusted, 3 = logistic adjusted
#   iter: required for cloglog adjusted output (iter can be increased if c obtained with not enough resolution)

fit_maxentP <- function(name,tag="",features="lq",PAbg=T,addsamp=T,outType=1,iter=10,nameout="MAXENT") { 

  # load datasets
  load(paste(path.datasets, "/", name, sep = ""))
  R1 <- length(M) 
  R2 <- length(M[[1]])
  
  # number of covarites depends on the factor missing (true or false)
  p<-ifelse(ident$missing == "F",5,4)
  
  # remind user of choices and build arguments for maxent
  cat("maxent: features =", features, ", addsamptobg =", addsamp, ", outType =", outType, "\n")
  myargs<-vector()  #default
  if(features=="lq")  myargs <- c("noautofeature", "nohinge", "nothreshold", "noproduct")
  if(features=="lqh") myargs <- c("noautofeature", "nothreshold", "noproduct")
  if(addsamp==F) myargs<-c(myargs,"-d")
  
  # return AUC, COR and RMSE
  AUC <- matrix(NA, ncol = R1, nrow = R2)
  COR <- matrix(NA, ncol = R1, nrow = R2)
  RMSE <- matrix(NA, ncol = R1, nrow = R2)
 
  # loop through all data sets available  
  thepreds<-list()   # to save predictions
  for (k in 1:R1) {
    thepreds[[k]]<-list()
    for (r in 1:R2) {
      
      ## prepare training data 
      A <- data.frame(pa = M[[k]][[r]]$pa, M[[k]][[r]][, 3:(2 + p)]) # depends on p
      if (PAbg==T){
        Abg <- A  #background is the presence-absence sample
      }else{
        bg <- bg.all[[k]][[r]]  #background is a random sample (a global variable generated by the function below)
        A <- A[A[,1]==1,]
        Abg <- rbind(A,bg)   #add presences to the data given to Maxent  
      }
      
      ## run Maxent with the options selected
      fit1 <- maxent(x = Abg[, -1], p = Abg[, 1], args = myargs)
      
      ## estimated prevalance from training data  
      prev.PA <- mean(M[[k]][[r]]$pa)              
      
      ## predict to test data (including post-processing of output to reflect probabilities)
      if (outType==0){  #default logistic, without adjusting (as per Thibaud et al)
        newpred <- predict(fit1, x = MT[[k]][[r]])
      }
      if (outType==1){  #raw adjusted
        rawpred <- predict(fit1,x=MT[[k]][[r]],args="outputformat=raw")    #raw predictions over test data
        numbg   <- fit1@results["X.Background.points",]    #number of points in background
        newpred <- rawpred*prev.PA*numbg    #adjusted raw output (probabilities)
        newpred[newpred>1] <- 1   #clipped to 1
      }
      if (outType==2){  #cloglog adjusted (find 'c' so that prevalence over predictions equals prev.PA )
        eta <- log(predict(fit1,x=M[[k]][[r]],args="outputformat=raw"))  # linear predictor from Maxent (training data)

        # Finding c by direct search to match observed prevalence (method proposed by Steven Phillips)
        cL<-ifelse(psi.estim(c=-5,eta=eta)>prev.PA,-20,-5) #select appropriate starting points for search
        cH<-ifelse(psi.estim(c=5,eta=eta)<prev.PA,20,5)
        c.estim <- search.c(cL=cL,cH=cH,prev=prev.PA,niter=iter,eta=eta)  #find best c
        
        # ALTERNATIVELY, find c as the intercept of a glm model with cloglog link and 'eta' as offset (method proposed by Bob O'Hara)
        #c.estim <- coef(glm(M[[k]][[r]]$pa ~ 1,  offset = eta,  family=binomial(link="cloglog")))
        
        etaT <- log(predict(fit1,x=MT[[k]][[r]],args="outputformat=raw")) #linear predictor from Maxent (test data)
        newpred <- 1-exp(-exp(etaT+c.estim))
      }   
      if (outType==3){  #logistic adjusted
        psi.logi <- predict(fit1, x = M[[k]][[r]])  #get default logistic output for training data (tau=0.5)
        tau.estim <- search.tau(tauL=0,tauH=1,prev=prev.PA,niter=10,psi.logi=psi.logi) #find best tau        
        psi.logi.T <- predict(fit1, x = MT[[k]][[r]])  #get default logistic output for test data (tau=0.5)
        newpred <- 1/(1+((1-tau.estim)/tau.estim)*((1-psi.logi.T)/psi.logi.T))  #map to chosen tau
      }
 
      ## evaluate performance          
      AUC[r, k] <- performance(prediction(newpred, labels = MT[[k]][[r]]$pa), measure = "auc")@y.values[[1]]
      COR[r, k] <- biserial.cor(x = newpred, y = MT[[k]][[r]]$pa, use = "all.obs", level = 2)
      RMSE[r, k] <- sqrt(mean((newpred - MT[[k]][[r]]$prob)^2))
      thepreds[[k]][[r]]<-newpred  
      
    }  #end for (r in 1:R2)
  }  #end for (k in 1:R1)
  
  ## gather things together to return  
  ident$technique <- nameout
  name.new <- paste(ident$num, "_", ident$missing, "_", ident$SAC, "_", ident$technique, "_", ident$samplesize, "_", ident$samplingbias,tag, ".Rdata", sep = "")
  save(AUC, RMSE, COR, ident, thepreds, file = paste(path.results, "/", name.new, sep = ""))   
  return(sum(is.na(AUC)))
}

#---------------------------------------------------------------------------------------------------------
# additional functions
#---------------------------------------------------------------------------------------------------------
# function that calculates the mean of the cloglog output, based on linear predictor 'eta' and constant 'c'
psi.estim <- function(c,eta) {mean(1-exp(-exp(eta+c)))}

# function to search for the value of 'c' in cloglog that approximates the observed prevalence in the PA sample
search.c <- function (cL,cH,prev,niter,eta){
  for (ii in 1:niter){
    cM <- cL+(cH-cL)/2   # find mid-value
    if( psi.estim(cM,eta) > prev){cH <- cM}
    else {cL <- cM} 
  }
  if(abs((psi.estim((cL+cH)/2,eta))-prev)>0.1) {print("WARNING: 'c' obtained with not enough resolution, increase n.iter")}
  return((cL+cH)/2)
}

# function that implements the logistic output, based on linear predictor 'eta[i]', the relative entropy 'r' and parameter 'tau'
psi.estim.logistic <- function(tau,psi.logi) {mean(1/(1+((1-tau)/tau)*((1-psi.logi)/psi.logi)))}

# function to search for the value of 'tau' that approximates the observed prevalence in the PA sample
search.tau <- function (tauL,tauH,prev,niter,psi.logi){
  for (ii in 1:niter){
    tauM <- tauL+(tauH-tauL)/2   # find mid-value
    if( psi.estim.logistic(tauM,psi.logi) > prev){tauH <- tauM}
    else {tauL <- tauM}
  }
  if(abs((psi.estim.logistic((tauL+tauH)/2,psi.logi))-prev)>0.1) {print("WARNING: 'tau' obtained with not enough resolution, increase n.iter")}
  return((tauL+tauH)/2)
}

# function to select random samples for background (stored as global variable)
MakeRandBG <- function(R1=10, R2=5, sampsize=10000){
  #set R1 and R2 for the number of realisations and samples used
  load(paste(path.species,"/","01_F.Rdata",sep=""))  #get info on predictors from any species file
  zz <- dim(L$pred)[1]
  bg.list <- list()
  for(i in 1:R1){
      bg.list[[i]] <- list()
      for(k in 1:R2){
        temp <- sample(1:zz,sampsize)
        bg.list[[i]][[k]] <- data.frame(pa=rep(0,sampsize), L$pred[temp,])
      } 
  }
  assign("bg.all", bg.list, envir = .GlobalEnv)
}


